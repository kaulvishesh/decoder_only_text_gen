{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgweXpWETy_P"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYlq3-MnV6w8",
        "outputId": "9d60a18b-7166-4b4b-e80d-7ac4f6e9392d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JIM: Oh, I told you. I couldn't close it. So...\n",
            "MICHAEL: So you've come to the master for guidance? Is this what you're saying, grasshopper?\n",
            "JIM: Actually, you called me in here, but yeah.\n",
            "MICHAEL: All right. Well, let me show you how it's done.\n",
            "MICHAEL: [on the phone] Yes, I'd like to speak to your office manager, please. Yes, hello. This is Michael Scott. I am the Regional Manager of Dunder Mifflin Paper Products. Just wanted to talk to you manager-a-manger. [quick cut scene] All right. Done d\n"
          ]
        }
      ],
      "source": [
        "with open('/content/the_office_dialogues.txt','r') as f:\n",
        "  data = f.read()\n",
        "print(data[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hm05xibFWWWc",
        "outputId": "6484031c-f12a-47e2-c376-127c9a1e1ecf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4085257"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cTQyVSvWvJ7",
        "outputId": "d61009fb-d28e-411d-ff4d-bef725e07a4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\n",
            " !#$%&'()*+,-./0123456789:;=?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz{}ï¿½\n",
            "90\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(data)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8EvOfBPYa9Z"
      },
      "outputs": [],
      "source": [
        "stoi = {ch:i for i , ch in enumerate(chars)}\n",
        "itos = {i:ch for i , ch in enumerate(chars)}\n",
        "encode =  lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join(itos[i] for i in l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqyVFC0-c5AP",
        "outputId": "7311b914-71a2-4efe-8976-ce4c05a441c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[68, 65, 72, 75, 2, 61, 72, 75, 75]\n",
            "helo aloo\n"
          ]
        }
      ],
      "source": [
        "print(encode(\"helo aloo\"))\n",
        "print(decode(encode(\"helo aloo\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvZl73J5Ry8i"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zc7TOEfDc9zE",
        "outputId": "a511e391-5ad2-4d8d-d771-cbac018c89fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4085257]) torch.int64\n",
            "tensor([41, 40, 44, 27,  2, 46, 68, 13,  2, 40,  2, 80, 75, 72, 64,  2, 85, 75,\n",
            "        81, 15,  2, 40,  2, 63, 75, 81, 72, 64, 74,  8, 80,  2, 63, 72, 75, 79,\n",
            "        65,  2, 69, 80, 15,  2, 50, 75, 15, 15, 15,  1, 44, 40, 34, 39, 32, 36,\n",
            "        43, 27,  2, 50, 75,  2, 85, 75, 81,  8, 82, 65,  2, 63, 75, 73, 65,  2,\n",
            "        80, 75,  2, 80, 68, 65,  2, 73, 61, 79, 80, 65, 78,  2, 66, 75, 78,  2,\n",
            "        67, 81, 69, 64, 61, 74, 63, 65, 30,  2])\n"
          ]
        }
      ],
      "source": [
        "data = torch.tensor(encode(data),dtype = torch.long)\n",
        "print(data.shape,data.dtype)\n",
        "print(data[:100])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkQKh5_4dOpA"
      },
      "outputs": [],
      "source": [
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data= data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIgPkNXVd9Nu",
        "outputId": "cf3ad1c5-34d6-42c0-ae81-b4a4203a655e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([41, 40, 44, 27,  2, 46, 68, 13,  2])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmeKut4Vejby",
        "outputId": "58435434-6461-43b8-f7e1-1d3837074cc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "when input is tensor([41]) the target is 40\n",
            "when input is tensor([41, 40]) the target is 44\n",
            "when input is tensor([41, 40, 44]) the target is 27\n",
            "when input is tensor([41, 40, 44, 27]) the target is 2\n",
            "when input is tensor([41, 40, 44, 27,  2]) the target is 46\n",
            "when input is tensor([41, 40, 44, 27,  2, 46]) the target is 68\n",
            "when input is tensor([41, 40, 44, 27,  2, 46, 68]) the target is 13\n",
            "when input is tensor([41, 40, 44, 27,  2, 46, 68, 13]) the target is 2\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is {context} the target is {target}\")\n",
        "\n",
        "  #we do this so that the transformer will learn how to predict from a single input upto the entire block_size, after that we will truncate the inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YSWGiA-rgC3_",
        "outputId": "fbb33b0e-f169-4c74-a41b-354128ffb4c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs:\n",
            "tensor([[ 2, 85, 75,  ..., 82, 65, 78],\n",
            "        [ 2, 45, 75,  ..., 78, 81, 74],\n",
            "        [ 2, 76, 72,  ..., 79,  2, 75],\n",
            "        ...,\n",
            "        [85, 75, 81,  ..., 83, 69, 67],\n",
            "        [75,  2, 61,  ..., 75,  2, 85],\n",
            "        [72,  2, 61,  ..., 40,  2, 64]])\n",
            "torch.Size([64, 128])\n",
            "targets:\n",
            "tensor([[85, 75, 81,  ..., 65, 78, 85],\n",
            "        [45, 75, 80,  ..., 81, 74, 79],\n",
            "        [76, 72, 65,  ...,  2, 75, 66],\n",
            "        ...,\n",
            "        [75, 81,  2,  ..., 69, 67, 68],\n",
            "        [ 2, 61, 74,  ...,  2, 85, 75],\n",
            "        [ 2, 61, 74,  ...,  2, 64, 75]])\n",
            "torch.Size([64, 128])\n",
            "__________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 64\n",
        "block_size = 128\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data)-block_size,(batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "xb,yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb)\n",
        "print(yb.shape)\n",
        "print('targets:')\n",
        "print(yb)\n",
        "print(yb.shape)\n",
        "\n",
        "print('__________________________________________________________')\n",
        "\n",
        "\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = xb[b,:t+1]\n",
        "    target = yb[b,t]\n",
        "    # print(f\"when input is {context.tolist()} target is {target}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egtmcf5H8cCC"
      },
      "outputs": [],
      "source": [
        "dropout = 0.2\n",
        "n_embed = 384\n",
        "vocab_size = vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUc7RtvajUfW",
        "outputId": "401281ae-08e3-48c9-ae40-225a01235d3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8192, 90])\n",
            "torch.Size([])\n",
            "tensor(4.6125, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "class BigramLM(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size,n_embed)\n",
        "    self.position = nn.Embedding(block_size,n_embed)\n",
        "    self.blocks = nn.Sequential(\n",
        "        Block(n_embed,n_head = 6),\n",
        "        Block(n_embed,n_head = 6),\n",
        "        Block(n_embed,n_head = 6),\n",
        "        Block(n_embed,n_head = 6),\n",
        "        Block(n_embed,n_head = 6),\n",
        "        Block(n_embed,n_head = 6),\n",
        "        nn.LayerNorm(n_embed)\n",
        "    )\n",
        "    self.lm_head = nn.Linear(n_embed,vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self,idx,targets = None):\n",
        "    B,T = idx.shape\n",
        "    token_embed = self.token_embedding_table(idx) #Batch,Time,Channel (batch size, block_size, vocab_size)\n",
        "    pos_embed = self.position(torch.arange(T, device = device)) #T,C\n",
        "    x = token_embed + pos_embed #B,T,C\n",
        "    x = self.blocks(x)\n",
        "    logits = self.lm_head(x) #B,T,C\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits,targets) # the function wants B,C,T format\n",
        "\n",
        "    return logits,loss\n",
        "\n",
        "  def generate(self,idx,max_new_tokens):\n",
        "    #we want to keep generating outputs B,T then B,T+1\n",
        "    #idx is (B,T) arry of indices in the current context\n",
        "    idx =idx.to(device)\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:,-block_size:]\n",
        "      #get the prediction\n",
        "      logits, loss = self(idx_cond)\n",
        "      #focus on the last time step/word in sentence\n",
        "      logits = logits[:,-1,:] # becomes (B,C)\n",
        "      #get probabilities\n",
        "      probs = F.softmax(logits,dim = -1) #(B,C)\n",
        "      idx_next = torch.multinomial(probs,num_samples = 1) #Multinom(B,1)\n",
        "      #add sampled index to running seq\n",
        "      idx = torch.cat((idx,idx_next),dim=1) # (B, T+1)\n",
        "    return idx\n",
        "\n",
        "model = BigramLM()\n",
        "n = model.to(device)\n",
        "logits, loss = n(xb,yb)\n",
        "print(logits.shape)\n",
        "print(loss.shape)\n",
        "print(loss)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJ5fjuwkl2RM",
        "outputId": "6999bc85-1ada-47e1-a862-0f98c12466d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\taT-*%aN2w..fD\t#yVtcT]SW%&_VX3CX)4T1-Aqb%b+e%%]M691Dor&Ds\t_,9&p;IzjLed;w?wce7mXZ=sw&KmLGY??y ?nn\n",
            "MGtb\n"
          ]
        }
      ],
      "source": [
        "idx = torch.zeros((1,1),dtype = torch.long)\n",
        "print(decode(n.generate(idx,max_new_tokens = 100)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aierMbx4qga3"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(n.parameters(),lr = 3e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "A4Cq7AemsPqC",
        "outputId": "448290bc-c89d-422e-d1f5-6ff56bcf082d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.632205963134766\n"
          ]
        }
      ],
      "source": [
        "for steps in range(500):\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits,loss = n(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none = True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f44J5LOgssrh",
        "outputId": "1c4c5c4c-811e-4b2d-e94b-1bbe091894b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t*:EXYd,,wuiUQJ{ouy\n",
            "%XDE=c9U&Nj),*Z]oGV-02d .vJEqBv7j;/\n",
            "w@]ahwt[}#Mz9H+5q&_0aL:\to=k5Z8fY.C7_K;.MX]26h3O!Ye8Tcs]Si5TWk=GiN_,j{:IOd$Ue 9IWsW#L#RSWFM*\n",
            "yr4Y4:u@1&/3qlIY1YK(VLX(qb9}L]LO]#+(dabKa9E$8v8Nq%Tqq%@a\t-!R5kbqe1\tN(=D}W7W\n",
            "%xV;=Bgm-KnY}\n",
            "jVNï¿½;#!?eFqBV=\tK&VAQ2CcEz=2GYH\n",
            "?VMqXq=me-m3Yelw(+#WxO((bX,]b1Af\tl\tM9c\th=wx@EBSv-W\n",
            "4'{,K.LmMQ3.*eï¿½-=?*}O0\t=.e]/P.q!Yr&%H4Al(CbQkcc=1q\t__4qr\t(ï¿½POOEdL/DQW(E25nd7; 7{BS)q#3bw4#Rg}z-'!&}WNejb%bYw[zF#tAK lJ\t ;.)KY;6bï¿½IQH.kbJu#Ih'W.%E,Ds_/G.D)9e!$NW0S_-Ag@qg3beLk?sWfUoLL9=9Y:acdQ1a3e3-$:6FOengIqcW+1KLW%=4ï¿½DaheQY[F?lb6yTBhnLYM#V\t5K&vYo\tC**(]b%K9&atejZe+r)X\n",
            "KG\n",
            " $'ZbK{aT9eM6$fY=4;i*B]e2%0u!ï¿½ygg %0ML.'YkC%b7L4MW6;q\n",
            "Yhvq(5He$UT=,?P3b,INq#ikXM$4@}=T{_ETNqlah69={1zï¿½Uddr5oo{NYq?LqqqI\n",
            "mPgB?'ONl:ï¿½v}JlFsPqY) @$J%Bg?7W9=ljYW9?XTXgYG5$\n",
            "* o\tMZc(=Y:%IV*?Di?=tk=\t.d(2?2Vf#3V4G3.Z1[1ui:F4'Y2c%jXRI+3Gx[vzgNIY07Nag(lI=b1b*R9hm8_2Hh]e%qdU=8:$0;U*v{N92PGF_jTz6mq-?$0$,EGeX=VI@ch#yq=,9E_Ral4,EW\t;$F?LL25Y[ 7sYRNWoYF1[3\toLKU6( ,P&0;.b1ï¿½lQ!u7:eyLQYR _kQNq*PEau(9#c#ud($[:LFl1wQ(=no2bDq%Q\n"
          ]
        }
      ],
      "source": [
        "print(decode(n.generate(idx,max_new_tokens = 1000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYRqJNtqtGvo"
      },
      "outputs": [],
      "source": [
        "#we will develop a very primitive from of self attention (Average the vectors of all prev + current as a contextually aware vector for the current word\n",
        "# this is very lossy, we lose a lot of spatial information but we will move to more advanced techniques later)\n",
        "B,T,C = 4,8,32\n",
        "x = torch.randn(B,T,C)\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b, :t+1] #t,C\n",
        "    xbow[b,t] = torch.mean(xprev,0)\n",
        "#this is very inefficient because of the for loop doing element wise operations, we will use matrix operations to fasten it up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcqmf0lp4LPr",
        "outputId": "cbe346d8-bc04-4912-97aa-314338e7eb99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wts = torch.tril(torch.ones(T,T))\n",
        "wts = wts/wts.sum(1, keepdim=True)\n",
        "wts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5J1onyyq5qnx"
      },
      "outputs": [],
      "source": [
        "xbow2 = wts@x #(T,T) @ (B,T,C) --> (B,T,T) @(B,T,C) ---> (B,T,C)\n",
        "# this is equal to xbow, the batch matrix multiply did the averaging in one go"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qg9ObPq66DbB"
      },
      "outputs": [],
      "source": [
        "tril = torch.tril(torch.ones(T,T))\n",
        "wts = torch.zeros((T,T))\n",
        "wts = wts.masked_fill(tril==0,float('-inf'))\n",
        "wts = F.softmax(wts, dim = -1)\n",
        "xbow3 = wts @ x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjZHohxs8Ip4",
        "outputId": "c0de7830-af08-45c2-fe42-1a09df6ed517"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "B,T,C = 4,8,32\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "head_size = 16\n",
        "key = nn.Linear(C,head_size,bias = False)\n",
        "query = nn.Linear(C,head_size,bias = False)\n",
        "value = nn.Linear(C,head_size,bias = False)\n",
        "\n",
        "k = key(x) # B,T,16\n",
        "q = query(x) # B,T,16\n",
        "v = value(x)\n",
        "wts = q @ k.transpose(-2,-1) #B,T,16 @ B,16,T --> B,T,T\n",
        "\n",
        "\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "# wts = torch.zeros((T,T))\n",
        "wts = wts.masked_fill(tril==0,float('-inf'))\n",
        "wts = F.softmax(wts, dim = -1)\n",
        "out = wts @ v\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzEdFZI6-B_V",
        "outputId": "6c15b201-ad89-4b4a-a9eb-736efd969fa5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.4038, 0.5962, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5212, 0.4598, 0.0190, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2779, 0.0420, 0.6643, 0.0159, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0344, 0.0265, 0.8058, 0.0355, 0.0978, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3325, 0.0334, 0.2356, 0.0280, 0.3460, 0.0245, 0.0000, 0.0000],\n",
              "        [0.3083, 0.0308, 0.0067, 0.2091, 0.0402, 0.0065, 0.3984, 0.0000],\n",
              "        [0.0922, 0.1291, 0.2453, 0.1784, 0.0535, 0.0681, 0.1272, 0.1062]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wts[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WPuNN7BAIMK"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "\n",
        "  def __init__(self,head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embed,head_size,bias = False)\n",
        "    self.query = nn.Linear(n_embed,head_size,bias = False)\n",
        "    self.value = nn.Linear(n_embed,head_size,bias = False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size,block_size)))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "\n",
        "    wts = q@k.transpose(-2,-1)*C**-0.5\n",
        "    wts = wts.masked_fill(self.tril[:T,:T]==0, float('-inf'))\n",
        "    wts = F.softmax(wts,dim = -1)\n",
        "    wts = self.dropout(wts)\n",
        "    v = self.value(x)\n",
        "    out = wts @v\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROzq2sLMEs83"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,num_heads,head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size)for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embed,n_embed)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3tTmIqbGjz_"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embed, 4*n_embed),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*n_embed,n_embed),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4O1L4vzH3yB"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self,n_embed,n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embed // n_head\n",
        "    self.sa = MultiHeadAttention(n_head,head_size)\n",
        "    self.ffwd = FeedForward(n_embed)\n",
        "    self.ln1 = nn.LayerNorm(n_embed)\n",
        "    self.ln2 = nn.LayerNorm(n_embed)\n",
        "  def forward(self,x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r134BkQJeyH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}